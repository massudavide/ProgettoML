{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "#from autoaugment import ImageNetPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if it's available\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# nel mio caso non dispongo di abbastanza RAM nella GPU per un'elaborazione su scheda grafica\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_file = r'/home/davide/Documents/Progetto/dataset/FoodSplitted'\n",
    "grey_hist = r'/home/davide/Documents/Progetto/dataset/grey_hist_coversion'\n",
    "\n",
    "data_dir = grey_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the pretrained models require the input to be 224x224 images. Also, we'll need to match the normalization used when the models were trained. Each color channel was normalized separately, the means are `[0.485, 0.456, 0.406]` and the standard deviations are `[0.229, 0.224, 0.225]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'/home/davide/Documents/Progetto/dataset/grey_hist_coversion'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                        [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_data = datasets.ImageFolder(data_dir + r'/train', transform=transform)\n",
    "val_data = datasets.ImageFolder(data_dir + r'/val', transform=transform)\n",
    "test_data=datasets.ImageFolder(data_dir + r'/test', transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=6)\n",
    "test_loader=torch.utils.data.DataLoader(test_data, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # TODO: Define transforms for the training data and testing data\n",
    "# train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "#                                        transforms.RandomResizedCrop(224),\n",
    "#                                        transforms.RandomHorizontalFlip(),\n",
    "#                                        #ImageNetPolicy(),\n",
    "#                                        transforms.ToTensor(),\n",
    "#                                        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                                                             [0.229, 0.224, 0.225])])\n",
    "\n",
    "# test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "#                                       transforms.CenterCrop(224),\n",
    "#                                       transforms.ToTensor(),\n",
    "#                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "#                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "# # Pass transforms in here, then run the next cell to see how the transforms look\n",
    "# train_data = datasets.ImageFolder(data_dir + r'/train', transform=train_transforms)\n",
    "# val_data = datasets.ImageFolder(data_dir + r'/val', transform=test_transforms)\n",
    "# test_data=datasets.ImageFolder(data_dir + r'/test', transform=test_transforms)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_data, batch_size=128)\n",
    "# test_loader=torch.utils.data.DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =models.densenet121(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "\n",
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(1024, 500)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(500, 101)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "    \n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "adam_optimizer = optim.Adam(model.classifier.parameters(), lr=0.001, betas=[0.9, 0.999])\n",
    "\n",
    "RMSProp_optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "def train(n_epochs,trainloader,testloader, model, optimizer, criterion, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "  \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        print('\\n\\t-----------------\\n')\n",
    "        print(start,'\\n')\n",
    "        \n",
    "        running_loss=0\n",
    "        model.train() \n",
    "        \n",
    "        for inputs, labels in trainloader:\n",
    "        \n",
    "            # Move input and label tensors to the default device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            \n",
    "            # we need to set the gradients to zero before starting to do backpropragation\n",
    "            # because PyTorch accumulates the gradients on subsequent backward passes.\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            logps = model(inputs)\n",
    "            loss = criterion(logps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print('running_loss: ', running_loss)\n",
    "        print(f\"time trainloader: {(time.time() - start):.3f} seconds\\n\")    \n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss=0\n",
    "        accuracy=0\n",
    "        with torch.no_grad():\n",
    "            # Disabling gradient calculation is useful for inference,\n",
    "            # when you are sure that you will not call Tensor.backward(). \n",
    "            # It will reduce memory consumption for computations \n",
    "            # that would otherwise have requires_grad=True.\n",
    "            \n",
    "            for inputs, labels in testloader:\n",
    "                # inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                logps = model(inputs)\n",
    "                batch_loss = criterion(logps, labels)\n",
    "                valid_loss += batch_loss.item()\n",
    "                    \n",
    "                # Calculate accuracy\n",
    "                \n",
    "                top_p, top_class = logps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                \n",
    "        \n",
    "            if valid_loss < valid_loss_min:\n",
    "                print(\"Validation loss decreased. Saving model\")\n",
    "                torch.save(model.state_dict(),save_path)\n",
    "                valid_loss_min=valid_loss\n",
    "                \n",
    "                    \n",
    "            \n",
    "            print(f\"Device = {device} ; Time per batch: {(time.time() - start):.3f} seconds\")       \n",
    "            print(f\"Epoch: {epoch+1}/{n_epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/len(trainloader):.3f}.. \"\n",
    "                  f\"Test loss: {valid_loss/len(testloader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\\n\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('denseNet_food101.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(5,train_loader,val_loader, model, RMSProp_optimizer, criterion,'denseNet_food101_5_220621_RMSProp.pt')\n",
    "# train(5,trainloader,testloader, model, sgd_optimizer, criterion,'denseNet_food101_5_sgd.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(),'denseNet_food101_10_noTran_220621_adam.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print('Training these layers')\n",
    "for name,param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "    if name.startswith('features.denseblock4.denselayer16'):\n",
    "        param.requires_grad = True\n",
    "#         print('---->', name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('denseNet_food101_5_noTran_220621_RMSProp.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(5,train_loader,val_loader, model, adam_optimizer, criterion,'denseNet_food101_5_rmsporp_5_adam_220621.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'denseNet_food101_5_rmsprop_5_adam.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('denseNet_food101.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss=0\n",
    "accuracy=0\n",
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  for images,labels in test_loader:\n",
    "    # images,lables=images.cuda(),labels.cuda()\n",
    "    images,lables=images.to(device),labels.to(device)\n",
    "    logps = model(images)\n",
    "    batch_loss = criterion(logps, labels)\n",
    "    valid_loss += batch_loss.item()\n",
    "    top_p, top_class = logps.topk(1, dim=1)\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "print(valid_loss/len(test_loader))\n",
    "print(accuracy/len(test_loader))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
